"""
LLM Helper Module

This module handles all interactions with OpenAI's API.
It's used to:
1. Interpret quiz questions
2. Solve quiz tasks using AI reasoning
3. Test prompt security
"""

from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

class LLMHelper:
    def __init__(self):
        """
        Initialize the LLM Helper with OpenAI API key.
        """
        self.api_key = os.getenv("OPENAI_API_KEY")
        
        # Create OpenAI client (new format - not openai.ChatCompletion)
        self.client = OpenAI(api_key=self.api_key)
        
        # Use GPT-4 Turbo (or GPT-4o if available)
        self.model = "gpt-4-turbo"
    
    def interpret_question(self, question_text):
        """
        Use LLM to understand what the quiz question is asking.
        
        Example:
            Question: "Download PDF from link. What is sum of column X?"
            LLM Returns: "Task: Extract data from PDF, parse table, calculate sum"
        """
        prompt = f"""
You are a data analysis expert. Read this quiz question carefully and break it down:

QUESTION:
{question_text}

Please identify:
1. What task needs to be done (web scraping, API call, PDF analysis, calculation, chart generation, etc.)
2. What data sources are mentioned or needed
3. What format the answer should be in (number, string, boolean, JSON, image, etc.)
4. What steps are required

Be clear and concise.
"""
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful data analysis expert. Break down tasks clearly."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3  # Lower = more consistent, higher = more creative
        )
        
        return response.choices[0].message.content
    
    def solve_quiz(self, question_text, available_data):
        """
        Use LLM to solve the actual quiz question.
        
        This is the MAIN problem-solving function.
        
        Args:
            question_text: The quiz question from the HTML page
            available_data: Dictionary with downloaded files/API responses
        
        Returns:
            The answer (number, string, etc.)
        """
        
        # Format the available data for the LLM
        data_summary = self._format_data_for_llm(available_data)
        
        prompt = f"""
You are an expert data analyst. Solve this quiz question step by step.

QUIZ QUESTION:
{question_text}

AVAILABLE DATA:
{data_summary}

Instructions:
1. Carefully read and understand what's being asked
2. Use the provided data to find the answer
3. Show your reasoning step by step
4. Be precise with calculations
5. At the end, state your final answer clearly

Format your response as:
REASONING:
[Your step-by-step reasoning]

FINAL ANSWER:
[The answer only - no explanation here]
"""
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert data analyst. Provide accurate answers with clear reasoning."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3
        )
        
        output = response.choices[0].message.content
        
        # Extract the final answer from LLM response
        answer = self._extract_answer(output)
        
        print(f"\nðŸ“Š LLM Reasoning:\n{output}")
        print(f"\nâœ¨ Extracted Answer: {answer}")
        
        return answer
    
    def _format_data_for_llm(self, available_data):
        """
        Format the downloaded/collected data in a readable way for the LLM.
        """
        if not available_data:
            return "No data provided"
        
        formatted = ""
        for key, value in available_data.items():
            # Limit size to avoid token limits
            if isinstance(value, str) and len(value) > 5000:
                formatted += f"\n{key}:\n{value[:5000]}... [truncated]\n"
            else:
                formatted += f"\n{key}:\n{value}\n"
        
        return formatted
    
    def _extract_answer(self, llm_output):
        """
        Extract the final answer from the LLM response.
        
        Looks for "FINAL ANSWER:" section and extracts what comes after it.
        """
        lines = llm_output.split('\n')
        
        # Find the line with "FINAL ANSWER"
        for i, line in enumerate(lines):
            if "FINAL ANSWER" in line.upper():
                # Get the next line(s) as the answer
                if i + 1 < len(lines):
                    answer = lines[i + 1].strip()
                    return answer
        
        # If no FINAL ANSWER section, return last non-empty line
        for line in reversed(lines):
            if line.strip():
                return line.strip()
        
        return None
    
    def test_prompt_security(self, system_prompt, user_prompt, code_word):
        """
        Test if your system prompt can hide a code word against user prompt.
        
        This is for the PROMPT TESTING component of the project.
        
        Args:
            system_prompt: Your system prompt (from .env or Google Form)
            user_prompt: User prompt trying to extract the code word
            code_word: Random word to hide/reveal
        
        Returns:
            Dictionary with:
            - revealed: True if code word was revealed, False if hidden
            - output: The LLM's response
        """
        
        messages = [
            {
                "role": "system",
                "content": f"{system_prompt} The code word is: {code_word}"
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7
        )
        
        output = response.choices[0].message.content
        
        # Check if code word appears in output (case-insensitive, ignore punctuation)
        output_lower = output.lower()
        code_word_lower = code_word.lower()
        
        # Remove punctuation for matching
        import re
        output_cleaned = re.sub(r'[^\w\s]', '', output_lower)
        code_word_cleaned = re.sub(r'[^\w\s]', '', code_word_lower)
        
        revealed = code_word_cleaned in output_cleaned
        
        return {
            "revealed": revealed,
            "output": output,
            "code_word": code_word
        }

